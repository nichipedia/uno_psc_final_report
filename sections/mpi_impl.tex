\section{MPI Implementation}
The MPI implementation is another pure C implementation that follows the parallel computational framework we developed. 
The parallel portion of this code relies on the number of processes set at program execution. 
Computation is divided up based on each process being responsible for performing computations on a predetermined number of grid rows. 
MPI\_Bcast was used to share input data across processes and MPI\_Gather was used to receive computation results from each process. 
Individual functions were written to compute mean and standard deviation based on an input grid. 
An integer input parameter was used to signal which type of statistical computation to perform.

\par
The initial design of this implementation was to use a single process to read the input data and use the MPI\_Bcast method to pass the entire grid to each process. 
The though was if each process could contain the entire grid, this would limit the amount of data exchange that would need to take place across active processes. 
However, during execution, the MPI\_Bcast would never complete causing the application to hang. 
We attributed this behavior to the size of the data that was being transmitted.

\par
The second attempt was to let each process read the input grid data. 
While this approach is inefficient, we felt that this was a suitable workaround to the MPI\_Bcast issue. 
This solution was able to process statistical computations but the performance was extremely poor. 
To run the MPI implementation we used the Louisiana Optical Network Infrastructure (LONI) resources. 
LONI sets limitations on the amount of time an application can execute. 
Due to the poor performance of our MPI implementation our processes would be terminated for exceeding these thresholds and a complete output data set would not be generated. 
It is our hypothesis that the large amounts of data being sent between processes, particularly through the MPI\_Gather function, was the cause for the poor performance.

\par
Going forward we would like to develop more sophisticated algorithms that can reduce the amount of data being sent across processes. 
There is potential to preprocess the data by segments using MPI\_Reduce for data that would need to be shared between computing processes. 
This may help to alleviate the amount of message passing needed. 
However, based on our initial findings, distributed memory parallelism does not seem to be well suited for this type of problem.